{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c1e8bd-e299-4928-9a9c-2632d5e26ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict1_2 = {\n",
    "    {\n",
    "        md:{n_warmups:{n_chains:{} for n_chains in N_CHAINS} for n_warmups in N_WARMUPS} for md in MAX_DOUBLINGS\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be924b-eeab-4d1b-8382-45941612b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sanity compare with the other way we do multi-chain sampling, should give same timing if I understand vectorization correctly\n",
    "print('md, n_warmups, n_chains')\n",
    "\n",
    "# no way to jit over the n_warmup or max_num_doublings\n",
    "for md in MAX_DOUBLINGS:\n",
    "    for n_warmups in N_WARMUPS:\n",
    "        state = results_dict['warmup'][md][n_warmups]['init_states']\n",
    "        tuned_params = results_dict['warmup'][md][n_warmups]['tuned_params']\n",
    "\n",
    "        logdensity = partial(_logprob_fn, data=data_gpu)\n",
    "        kernel = blackjax.nuts(logdensity, **tuned_params).step\n",
    "        for n_chains in N_CHAINS:\n",
    "            print(md, n_warmups, n_chains)\n",
    "            init_states = [state for ii in range(n_chains)] # CHECK\n",
    "            keys = random.split(rng_key, n_chains)\n",
    "            _run_inference = jax.jit(partial(inference_loop_multiple_chains, \n",
    "                                             n_samples = 1000 // n_chains, # fair runtime comparison\n",
    "                                             n_chains=n_chains, \n",
    "                                             kernel=kernel, \n",
    "                                            ))\n",
    "\n",
    "            # compilation time\n",
    "            t1 = time.time()\n",
    "            _ = _run_inference(keys, init_states)\n",
    "            t2 = time.time()\n",
    "            results_dict1_2['inference'][md][n_warmups][n_chains]['comp_time'] = t2 - t1 \n",
    "\n",
    "            # run time\n",
    "            t1 = time.time()\n",
    "            states, info = _run_inference(keys, init_states)\n",
    "            t2 = time.time()\n",
    "            results_dict1_2['inference'][md][n_warmups][n_chains]['run_time'] = t2 - t1\n",
    "            \n",
    "            # save states and info for future reference\n",
    "            results_dict1_2['inference'][md][n_warmups][n_chains]['states'] = states\n",
    "            results_dict1_2['inference'][md][n_warmups][n_chains]['info'] = info\n",
    "            \n",
    "            \n",
    "print()\n",
    "print('DONE!')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpd_gpu2",
   "language": "python",
   "name": "bpd_gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
